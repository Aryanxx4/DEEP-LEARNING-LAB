{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a316e5cc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "# 1D CNN for IMDB text classification\n",
        "\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f5ad14c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(50000, 50000)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load IMDB CSV from Data folder\n",
        "\n",
        "csv_path = './Data/IMDB Dataset.csv'\n",
        "\n",
        "# The CSV has columns: 'review', 'sentiment'\n",
        "df = pd.read_csv(csv_path)\n",
        "print(df.head())\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Map labels to integers\n",
        "label_map = {'negative': 0, 'positive': 1}\n",
        "df['label'] = df['sentiment'].map(label_map)\n",
        "\n",
        "texts = df['review'].astype(str).tolist()\n",
        "labels = df['label'].astype(int).tolist()\n",
        "\n",
        "len(texts), len(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a8271ab1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab size: 70386\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(50000, 200)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Text cleaning and tokenization, vocabulary and sequence building\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML breaks\n",
        "    text = text.replace('<br />', ' ')\n",
        "    # Keep letters and basic punctuation, replace others with space\n",
        "    text = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
        "    # Collapse multiple spaces\n",
        "    text = re.sub(r\"\\s+\", ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def tokenize(text: str):\n",
        "    return clean_text(text).split()\n",
        "\n",
        "# Build vocabulary\n",
        "min_freq = 2\n",
        "word_freq = {}\n",
        "for t in texts:\n",
        "    for tok in tokenize(t):\n",
        "        word_freq[tok] = word_freq.get(tok, 0) + 1\n",
        "\n",
        "# Reserve 0 for PAD, 1 for UNK\n",
        "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "for word, freq in word_freq.items():\n",
        "    if freq >= min_freq:\n",
        "        word2idx[word] = len(word2idx)\n",
        "\n",
        "idx2word = {idx: w for w, idx in word2idx.items()}\n",
        "vocab_size = len(word2idx)\n",
        "print('Vocab size:', vocab_size)\n",
        "\n",
        "max_len = 200  # truncate / pad length\n",
        "\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [word2idx.get(tok, word2idx['<UNK>']) for tok in tokens]\n",
        "    # Truncate\n",
        "    ids = ids[:max_len]\n",
        "    # Pad\n",
        "    if len(ids) < max_len:\n",
        "        ids += [word2idx['<PAD>']] * (max_len - len(ids))\n",
        "    return ids\n",
        "\n",
        "all_encoded = [encode(t) for t in texts]\n",
        "\n",
        "len(all_encoded), len(all_encoded[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "152a0d85",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(40000, 10000)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train/validation split and Dataset/DataLoader (no sklearn)\n",
        "\n",
        "X = np.array(all_encoded, dtype=np.int64)\n",
        "y = np.array(labels, dtype=np.int64)\n",
        "\n",
        "# Shuffle indices and split 80/20\n",
        "num_samples = len(X)\n",
        "indices = np.arange(num_samples)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(0.8 * num_samples)\n",
        "train_idx = indices[:split]\n",
        "val_idx = indices[split:]\n",
        "\n",
        "X_train, X_val = X[train_idx], X[val_idx]\n",
        "y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "train_dataset = IMDBDataset(X_train, y_train)\n",
        "val_dataset = IMDBDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9c6296f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextCNN(\n",
              "  (embedding): Embedding(70386, 128, padding_idx=0)\n",
              "  (convs): ModuleList(\n",
              "    (0): Conv1d(128, 100, kernel_size=(3,), stride=(1,))\n",
              "    (1): Conv1d(128, 100, kernel_size=(4,), stride=(1,))\n",
              "    (2): Conv1d(128, 100, kernel_size=(5,), stride=(1,))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1D CNN model definition\n",
        "\n",
        "class TextCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, kernel_sizes=(3,4,5), num_filters=100, dropout=0.5):\n",
        "        super(TextCNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embed_dim,\n",
        "                      out_channels=num_filters,\n",
        "                      kernel_size=k)\n",
        "            for k in kernel_sizes\n",
        "        ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len)\n",
        "        x = self.embedding(x)              # (batch, seq_len, embed_dim)\n",
        "        x = x.transpose(1, 2)              # (batch, embed_dim, seq_len)\n",
        "\n",
        "        conv_outs = []\n",
        "        for conv in self.convs:\n",
        "            c = torch.relu(conv(x))        # (batch, num_filters, L)\n",
        "            c = torch.max(c, dim=2).values # global max pool over time -> (batch, num_filters)\n",
        "            conv_outs.append(c)\n",
        "\n",
        "        x = torch.cat(conv_outs, dim=1)    # (batch, num_filters * len(kernel_sizes))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "num_classes = 2\n",
        "embed_dim = 128\n",
        "\n",
        "model = TextCNN(vocab_size=vocab_size,\n",
        "                embed_dim=embed_dim,\n",
        "                num_classes=num_classes).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "59aedb18",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: train_loss=0.6190, train_acc=0.6623, val_loss=0.4605, val_acc=0.7918\n",
            "Epoch 2: train_loss=0.4693, train_acc=0.7718, val_loss=0.4157, val_acc=0.8020\n",
            "Epoch 3: train_loss=0.4006, train_acc=0.8134, val_loss=0.3555, val_acc=0.8420\n",
            "Epoch 4: train_loss=0.3385, train_acc=0.8492, val_loss=0.3370, val_acc=0.8526\n",
            "Epoch 5: train_loss=0.2891, train_acc=0.8784, val_loss=0.3188, val_acc=0.8635\n"
          ]
        }
      ],
      "source": [
        "# Training and evaluation loop\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == y_batch).sum().item()\n",
        "        total += X_batch.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "def eval_epoch(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            total_loss += loss.item() * X_batch.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == y_batch).sum().item()\n",
        "            total += X_batch.size(0)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
