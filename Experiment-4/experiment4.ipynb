{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16b34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "with open(\"data/poems-100.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "tokens = text.split()\n",
    "vocab = sorted(set(tokens))\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c797c3d",
   "metadata": {},
   "source": [
    "ONE-HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37cfde33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    vec = torch.zeros(vocab_size)\n",
    "    vec[idx] = 1.0\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94c36b",
   "metadata": {},
   "source": [
    "Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a80dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 5\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(len(tokens) - seq_length):\n",
    "    seq = tokens[i:i+seq_length]\n",
    "    target = tokens[i+seq_length]\n",
    "    X.append([word2idx[w] for w in seq])\n",
    "    y.append(word2idx[target])\n",
    "\n",
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f1d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = X.shape[1]\n",
    "\n",
    "X_onehot = torch.zeros(X.shape[0], seq_length, vocab_size)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(seq_length):\n",
    "        X_onehot[i, j, X[i, j]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42f218f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "dataset = TensorDataset(X_onehot, y)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eff4344",
   "metadata": {},
   "source": [
    "RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "777d3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN_OneHot(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cdb6649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.4052\n",
      "Epoch 2, Loss: 6.7771\n",
      "Epoch 3, Loss: 6.4894\n",
      "Epoch 4, Loss: 6.1827\n",
      "Epoch 5, Loss: 5.9227\n",
      "Epoch 6, Loss: 5.6639\n",
      "Epoch 7, Loss: 5.4004\n",
      "Epoch 8, Loss: 5.1239\n",
      "Epoch 9, Loss: 4.8403\n",
      "Epoch 10, Loss: 4.5602\n",
      "Epoch 11, Loss: 4.2792\n",
      "Epoch 12, Loss: 4.0130\n",
      "Epoch 13, Loss: 3.7480\n",
      "Epoch 14, Loss: 3.4983\n",
      "Epoch 15, Loss: 3.2590\n",
      "Epoch 16, Loss: 3.0278\n",
      "Epoch 17, Loss: 2.8061\n",
      "Epoch 18, Loss: 2.5956\n",
      "Epoch 19, Loss: 2.3907\n",
      "Epoch 20, Loss: 2.1990\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "model = RNN_OneHot(vocab_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        output = model(xb)\n",
    "        loss = criterion(output, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b10ccd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the sun shines bright upon warmer than the chain for hate old the eyes of who days, the do you close waste more as you thought not me upon you and i am him i am the i of the old of my i were the them of my own in that them in my\n"
     ]
    }
   ],
   "source": [
    "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
    "num_words_to_generate = 50\n",
    "def generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length):\n",
    "    model.eval()\n",
    "    generated_words = list(seed_words)\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        if len(generated_words) < seq_length:\n",
    "            current_sequence_words = generated_words\n",
    "        else:\n",
    "            current_sequence_words = generated_words[-seq_length:]\n",
    "\n",
    "        try:\n",
    "            input_indices = [word2idx[w] for w in current_sequence_words]\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Word '{e.args[0]}' not in vocabulary. Skipping generation for this word.\")\n",
    "            break\n",
    "        input_tensor_indices = torch.tensor(input_indices).unsqueeze(0)\n",
    "        one_hot_input_tensor = torch.zeros(1, input_tensor_indices.shape[1], vocab_size, dtype=torch.float32)\n",
    "        for k, idx in enumerate(input_tensor_indices[0]):\n",
    "            one_hot_input_tensor[0, k, idx] = 1.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(one_hot_input_tensor)\n",
    "\n",
    "        predicted_idx = torch.argmax(output[:, -1, :]).item() if output.dim() == 3 else torch.argmax(output).item()\n",
    "        predicted_word = idx2word[predicted_idx]\n",
    "        generated_words.append(predicted_word)\n",
    "\n",
    "    return ' '.join(generated_words)\n",
    "generated_text = generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ead11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_OneHot(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(vocab_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4f016d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.4545\n",
      "Epoch 2, Loss: 6.8877\n",
      "Epoch 3, Loss: 6.7441\n",
      "Epoch 4, Loss: 6.5649\n",
      "Epoch 5, Loss: 6.3762\n",
      "Epoch 6, Loss: 6.1679\n",
      "Epoch 7, Loss: 5.9415\n",
      "Epoch 8, Loss: 5.6986\n",
      "Epoch 9, Loss: 5.4406\n",
      "Epoch 10, Loss: 5.1744\n",
      "Epoch 11, Loss: 4.9083\n",
      "Epoch 12, Loss: 4.6471\n",
      "Epoch 13, Loss: 4.3956\n",
      "Epoch 14, Loss: 4.1462\n",
      "Epoch 15, Loss: 3.9052\n",
      "Epoch 16, Loss: 3.6692\n",
      "Epoch 17, Loss: 3.4372\n",
      "Epoch 18, Loss: 3.2127\n",
      "Epoch 19, Loss: 2.9921\n",
      "Epoch 20, Loss: 2.7744\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 64\n",
    "model = LSTM_OneHot(vocab_size, hidden_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        output = model(xb)\n",
    "        loss = criterion(output, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9d5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the sun shines bright upon one the great of my voice, or me, i am the air and my old like for one to the little of the grass sails is a stars galleon, with an nor in in the earth, or not in a few or large or few where the few pleas'd with\n"
     ]
    }
   ],
   "source": [
    "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
    "num_words_to_generate = 50\n",
    "\n",
    "generated_text = generate_text_for_onehot(model, seed_words, num_words_to_generate, word2idx, idx2word, vocab_size, seq_length)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242d7c4",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa74129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = torch.tensor([[word2idx[w] for w in tokens[i:i+seq_length]]\n",
    "                  for i in range(len(tokens)-seq_length)])\n",
    "y = torch.tensor([word2idx[tokens[i+seq_length]]\n",
    "                  for i in range(len(tokens)-seq_length)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca8c95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb9cc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.4771\n",
      "Epoch 2, Loss: 6.5974\n",
      "Epoch 3, Loss: 6.2451\n",
      "Epoch 4, Loss: 5.9251\n",
      "Epoch 5, Loss: 5.6143\n",
      "Epoch 6, Loss: 5.3149\n",
      "Epoch 7, Loss: 5.0240\n",
      "Epoch 8, Loss: 4.7382\n",
      "Epoch 9, Loss: 4.4628\n",
      "Epoch 10, Loss: 4.1942\n",
      "Epoch 11, Loss: 3.9346\n",
      "Epoch 12, Loss: 3.6894\n",
      "Epoch 13, Loss: 3.4584\n",
      "Epoch 14, Loss: 3.2408\n",
      "Epoch 15, Loss: 3.0372\n",
      "Epoch 16, Loss: 2.8492\n",
      "Epoch 17, Loss: 2.6681\n",
      "Epoch 18, Loss: 2.5012\n",
      "Epoch 19, Loss: 2.3435\n",
      "Epoch 20, Loss: 2.1923\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "embedding_dataset = TensorDataset(X, y)\n",
    "embedding_loader = DataLoader(embedding_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "model = RNN_Embedding(vocab_size, embed_dim=100, hidden_size=64)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in embedding_loader:\n",
    "        output = model(xb)\n",
    "        loss = criterion(output, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(embedding_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8df6f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length):\n",
    "    model.eval()\n",
    "    generated_words = list(seed_words)\n",
    "\n",
    "    for _ in range(num_words_to_generate):\n",
    "        if len(generated_words) < seq_length:\n",
    "            current_sequence_words = generated_words\n",
    "        else:\n",
    "            current_sequence_words = generated_words[-seq_length:]\n",
    "        try:\n",
    "            input_indices = [word2idx[w] for w in current_sequence_words]\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Word '{e.args[0]}' not in vocabulary. Skipping generation for this word.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        input_tensor = torch.tensor(input_indices).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "\n",
    "\n",
    "        predicted_idx = torch.argmax(output[:, -1, :]).item() if output.dim() == 3 else torch.argmax(output).item()\n",
    "        predicted_word = idx2word[predicted_idx]\n",
    "        generated_words.append(predicted_word)\n",
    "\n",
    "    return ' '.join(generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4496d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the sun shines bright upon the masts and never chaws; by of my own, and gave you have them.” around me, a little birds in a row sat musing. a few quadrillions of the young men glisten'd the whole earth. of the moon and then the chaff for payment receiving, a few octillions of his\n"
     ]
    }
   ],
   "source": [
    "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
    "num_words_to_generate = 50\n",
    "\n",
    "generated_text = generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb550a7d",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b709cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "222fca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 7.4637\n",
      "Epoch 2, Loss: 6.7051\n",
      "Epoch 3, Loss: 6.4248\n",
      "Epoch 4, Loss: 6.1401\n",
      "Epoch 5, Loss: 5.8556\n",
      "Epoch 6, Loss: 5.5730\n",
      "Epoch 7, Loss: 5.2892\n",
      "Epoch 8, Loss: 5.0060\n",
      "Epoch 9, Loss: 4.7251\n",
      "Epoch 10, Loss: 4.4501\n",
      "Epoch 11, Loss: 4.1797\n",
      "Epoch 12, Loss: 3.9165\n",
      "Epoch 13, Loss: 3.6609\n",
      "Epoch 14, Loss: 3.4143\n",
      "Epoch 15, Loss: 3.1800\n",
      "Epoch 16, Loss: 2.9527\n",
      "Epoch 17, Loss: 2.7422\n",
      "Epoch 18, Loss: 2.5397\n",
      "Epoch 19, Loss: 2.3531\n",
      "Epoch 20, Loss: 2.1770\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Embedding(vocab_size, embed_dim=100, hidden_size=64)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for xb, yb in embedding_loader:\n",
    "        output = model(xb)\n",
    "        loss = criterion(output, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(embedding_loader):.4f}\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b05e8fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "the sun shines bright upon his own dashings—yet—the are scatter'd, in the woods are lovely, and their employments, and all times, and the woods i am the poet of the dying sun: ‘and the young men in the greatest and frozen lake of light! to-night it seems to me and i am not a few\n"
     ]
    }
   ],
   "source": [
    "seed_words = [\"the\", \"sun\", \"shines\", \"bright\", \"upon\"]\n",
    "num_words_to_generate = 50\n",
    "\n",
    "generated_text = generate_text(model, seed_words, num_words_to_generate, word2idx, idx2word, seq_length)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
